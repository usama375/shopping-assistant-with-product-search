{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "\n",
    "import yaml\n",
    "\n",
    "try:\n",
    "    # Load the config file\n",
    "    with open('config.yaml', 'rt') as f:\n",
    "        config = yaml.safe_load(f.read())\n",
    "except:\n",
    "    print(\"Got an Error  While Reading Config\")\n",
    "\n",
    "genai.configure(api_key=config['gemini-api-key'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils and Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "instructions_for_llm = \"\"\"\n",
    "You are a knowledgeable shoe recommendation assistant. Help the customer find the perfect shoes based on their preferences.\n",
    "\n",
    "Important Instructions:\n",
    "1. Ask interactive questions to understand user requirements fully\n",
    "2. When you have gathered enough information about user preferences, call the product recommendation function\n",
    "3. Keep your responses conversational but focused on gathering relevant information\n",
    "4. Once you have clear requirements, summarize them and use the perform_search_wrapper function\n",
    "5. Not only summarize the requirements but also add you best recommendation of shoes for customers requirements, \n",
    "    for example if a person asks for outdoor activiteis shoes,\n",
    "      you can add your best recommendation according to situation as \"Winter Hiking Boots\" or \"Hingh Ankle Boots\" etc\n",
    "\n",
    "Information to gather:\n",
    "- Event/Occasion\n",
    "- Primary use\n",
    "- Style preferences\n",
    "- Color preferences\n",
    "- Gender\n",
    "- Any specific requirements\n",
    "\n",
    "- Limit you response to 20 words only be specific\n",
    "\n",
    "When you have sufficient information, call the perform_search_wrapper function with a detailed but concise search query.\n",
    "Do not mention the function or its calling in your responses to the user.\n",
    "\"\"\"\n",
    "\n",
    "def perform_search_wrapper(text_query: str):\n",
    "    \"\"\"Call this function to look for the desired shoes.\n",
    "    \n",
    "    Args:\n",
    "        text_query: Input text query containing requirements of user for shoes curated by LLM\n",
    "    \"\"\"\n",
    "    print(\"\\nSearching for shoes with criteria:\", text_query)\n",
    "    # return {\"status\": \"success\", \"query\": text_query}\n",
    "\n",
    "# Function declaration for Gemini\n",
    "function_declarations = [{\n",
    "    \"name\": \"perform_search_wrapper\",\n",
    "    \"description\": \"Search for shoes based on user requirements. \\\n",
    "        - When creating parameter for function call don't add search for or find for in the querry  \",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"text_query\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Comprehensive search query based on user requirements\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"text_query\"]\n",
    "    }\n",
    "}]\n",
    "\n",
    "def initialize_chat():\n",
    "    \"\"\"Initialize the chat with the model and set up initial context\"\"\"\n",
    "    model = genai.GenerativeModel(\"gemini-pro\",\n",
    "                                generation_config={\n",
    "                                    \"temperature\": 0.7,\n",
    "                                    \"top_p\": 0.8,\n",
    "                                    \"top_k\": 40\n",
    "                                })\n",
    "    \n",
    "    chat = model.start_chat(history=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"parts\": [instructions_for_llm]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"model\",\n",
    "            \"parts\": [\"Hello! I'm your shoe shopping assistant. What type of shoes are you looking for today?\"]\n",
    "        }\n",
    "    ])\n",
    "    return chat\n",
    "\n",
    "def process_response(response):\n",
    "    \"\"\"Process the model's response and handle function calls\"\"\"\n",
    "# try:\n",
    "    function_call = response.candidates[0].content.parts[0].function_call\n",
    "    args = function_call.args\n",
    "    if function_call.name == \"perform_search_wrapper\":\n",
    "        # Parse arguments and call the function\n",
    "        # args = json.loads(function_call.args)\n",
    "        perform_search_wrapper(args[\"text_query\"])\n",
    "        return True\n",
    "# except Exception as e:\n",
    "    # print(f\"Error processing response: {str(e)}\")\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = initialize_chat()\n",
    "print(\"\\nWelcome to your Shoe Shopping Assistant! ðŸ‘Ÿ\")\n",
    "print(\"Tell me about what kind of shoes you're looking for, and I'll help you find the perfect pair!\")\n",
    "\n",
    "\n",
    "while True:\n",
    "    # try:\n",
    "    # Get user input\n",
    "    user_input =  str(input(\"\\nYou: \"))\n",
    "    if user_input.strip().lower() in ['quit', 'exit', 'bye']:\n",
    "        print(\"Thank you for shopping with us! Goodbye! ðŸ‘‹\")\n",
    "        break\n",
    "\n",
    "    response = chat.send_message(\n",
    "        user_input,\n",
    "        tools=[{\n",
    "            'function_declarations': function_declarations\n",
    "        }]\n",
    "    )\n",
    "    print(\"*********************\")\n",
    "    print(\"user_input \", user_input)\n",
    "    print(\"*********************\")\n",
    "    print(response)\n",
    "    print(\"*********************\")\n",
    "    # Process any function calls\n",
    "    has_function_call = process_response(response)\n",
    "    print(has_function_call)\n",
    "    if has_function_call:\n",
    "        print(\"\\nFound matching shoes based on your requirements! âœ¨\")\n",
    "        # Here you can add code to display actual product results\n",
    "    else:\n",
    "        print(\"*********************\")\n",
    "        print(response.text)\n",
    "        print(\"*********************\")\n",
    "\n",
    "# except Exception as e:\n",
    "    # print(f\"\\nAn error occurred: {str(e)}\")\n",
    "    print(\"Let's continue our conversation...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
