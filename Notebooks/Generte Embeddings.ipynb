{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from transformers import CLIPTokenizerFast, CLIPProcessor, CLIPModel\n",
    "import torch\n",
    "from PIL import Image\n",
    "import PIL\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing the CLIP model\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# if you have CUDA or MPS, set it to the active device like this\n",
    "device = \"cuda\" if torch.cuda.is_available() else \\\n",
    "         (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model_id = \"openai/clip-vit-base-patch32\"\n",
    "\n",
    "# we initialize a tokenizer, image processor, and the model itself\n",
    "tokenizer = CLIPTokenizerFast.from_pretrained(model_id)\n",
    "processor = CLIPProcessor.from_pretrained(model_id)\n",
    "model = CLIPModel.from_pretrained(model_id).to(device)\n",
    "\n",
    "# Get the cache directory for the model\n",
    "local_model_path = snapshot_download(repo_id=model_id)\n",
    "\n",
    "print(f\"Model Local Path: {local_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_image_paths(directory):\n",
    "    # Define a set of valid image extensions\n",
    "    valid_extensions = {'.jpg', '.jpeg', '.png', '.gif', '.bmp'}\n",
    "    # Get all files in the directory\n",
    "    all_files = os.listdir(directory)\n",
    "    file_path_dict = {}\n",
    "\n",
    "    for file in all_files:\n",
    "        if os.path.isfile(os.path.join(directory, file)) \\\n",
    "            and os.path.splitext(file)[1].lower() in valid_extensions:\n",
    "            file_path = os.path.join(directory, file)\n",
    "            file_path_dict[file] = file_path\n",
    "\n",
    "    return file_path_dict\n",
    "\n",
    "\n",
    "directory = \"images\"\n",
    "image_paths_dict = get_image_paths(directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Write dictionary to the file as JSON\n",
    "with open(\"paths_dict.json\", 'w') as file:\n",
    "    json.dump(image_paths_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(image_paths_dict.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def encode_text(prompt):\n",
    "\n",
    "  # create transformer-readable tokens\n",
    "  inputs = tokenizer(prompt, return_tensors=\"pt\")   # pt: it will return  pytorch sensors\n",
    "\n",
    "  # use CLIP to encode tokens into a meaningful embedding\n",
    "  text_emb = model.get_text_features(**inputs)\n",
    "\n",
    "  return text_emb\n",
    "\n",
    "\n",
    "def get_image_embd(path):\n",
    "  image = Image.open(path)\n",
    "  image = processor(\n",
    "      text=None,\n",
    "      images=image,\n",
    "      return_tensors='pt'\n",
    "  )['pixel_values'].to(device)\n",
    "  img_emb = model.get_image_features(image)\n",
    "\n",
    "  return img_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_rows_list = []\n",
    "for file in list(image_paths_dict.keys()):\n",
    "\n",
    "    embeddings = get_image_embd(image_paths_dict[file]).detach().numpy()[0]\n",
    "\n",
    "    db_row_dict = {\n",
    "        'product_name': file,\n",
    "        'image_embed': embeddings}\n",
    "\n",
    "    db_rows_list.append(db_row_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_storage = pd.DataFrame(db_rows_list)\n",
    "\n",
    "# Convert numpy arrays to string representation\n",
    "vector_storage['image_embed'] = vector_storage['image_embed'].apply(lambda x: np.array2string(x, separator=',')[1:-1])  # Remove brackets\n",
    "\n",
    "# Save DataFrame to CSV\n",
    "vector_storage.to_csv('vectors.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils for Search (Top K matching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def top_k_cosine_similarity(df, input_vector, k):\n",
    "    \"\"\"\n",
    "    Calculate cosine similarity with all vectors in the DataFrame and return the top k matches.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame containing product names and image embeddings.\n",
    "                           Schema: ['product_name', 'image_embed']\n",
    "        input_vector (list or np.array): Input vector for comparison.\n",
    "        k (int): Number of top matches to return.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing top k matching products with their similarity scores.\n",
    "    \"\"\"\n",
    "    # Convert input_vector to a 2D array (required for cosine_similarity)\n",
    "    input_vector = np.array(input_vector).reshape(1, -1)\n",
    "    \n",
    "    # Convert all image embeddings to a 2D numpy array\n",
    "    embeddings = np.vstack(df['image_embed'].values)\n",
    "    \n",
    "    # Compute cosine similarities\n",
    "    similarities = cosine_similarity(input_vector, embeddings).flatten()\n",
    "    \n",
    "    # Add similarity scores to the DataFrame\n",
    "    df['similarity'] = similarities\n",
    "    \n",
    "    # Get the top k matches based on similarity scores\n",
    "    top_k_matches = df.nlargest(k, 'similarity')\n",
    "    \n",
    "    # Drop the similarity column to keep the DataFrame clean (optional)\n",
    "    return top_k_matches[['product_name', 'similarity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('vectors.csv')\n",
    "\n",
    "# Convert string representation of vectors back to numpy arrays\n",
    "df['image_embed'] = df['image_embed'].apply(lambda x: np.array(ast.literal_eval(f'[{x}]')))  # Wrap string in brackets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text_querry =  encode_text(prompt=\"I am lookin for boots\").detach().numpy()[0]\n",
    "results = top_k_cosine_similarity(df=df, input_vector=text_querry, k=10)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "def display_results(file_names):\n",
    "    for file_name in file_names:\n",
    "        print(file_name)\n",
    "        im = Image.open(image_paths_dict[file_name])\n",
    "        display(im)\n",
    "display_results(results['product_name'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
